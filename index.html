
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="3D Shape Modeling and Reconstruction.">
    <meta name="author" content="Mingyue Yang, 
                                 Yuxin Wen, 
                                 Weikai Chen, 
                                 Yongwei Chen,
                                 Kui Jia">

    <title>Deep Optimized Priors for 3D Shape Modeling and Reconstruction</title>
    <!-- Bootstrap core CSS -->
    <link href="bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">

    <link rel="icon" href="img/1-ours.gif" type="image/gif" >
  </head>

  <body>
    <div class="container">

    <div class="jumbotron">
      <h2>Deep Optimized Priors for 3D Shape Modeling and Reconstruction</h2>
      <p class="abstract">3D Shape Modeling and Reconstruction</p>
      <!--<p iclass="authors">
          <a href="http://stanford.edu/~sitzmann/">Vincent Sitzmann</a>,
          <a href="https://niessnerlab.org/members/justus_thies/profile.html">Justus Thies</a>,
          <a href="https://scholar.google.com/citations?user=gRqzSHsAAAAJ&hl=en&oi=ao">Felix Heide</a>,
          <a href="https://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Niessner</a>,
          <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>,
          <a href="http://zollhoefer.com/">Michael Zollh√∂fer</a>
      </p>-->

      <p>
        <a class="btn btn-primary" href="#">Code</a>
        <a class="btn btn-primary" href="#">Dataset</a>
        <a class="btn btn-primary" href="https://arxiv.org/abs/2012.07241">Paper</a>
        <a class="btn btn-primary" href="https://drive.google.com/file/d/1BnZRyNcVUty6-LxAstN83H79ktUq8Cjp/view?usp=sharing">Supplemental Material</a> </p>
    </div>

    <div class="section">
        <h3>Abstract</h3>
        <hr>
        <p>
           Many learning-based approaches have difficulty scaling to unseen data, 
          as the generality of its learned prior is limited to the scale and variations of the training samples. 
          This holds particularly true with 3D learning tasks, given the sparsity of 3D datasets available. 
          We introduce a new learning framework for 3D modeling and reconstruction that greatly improves the generalization 
          ability of a deep generator. Our approach strives to connect the good ends of both learning-based and optimization-based methods. 
          In particular, unlike the common practice that fixes the pre-trained priors at test time, we propose to further optimize the learned
          prior and latent code according to the input physical measurements after the training. We show that the proposed strategy effectively 
          breaks the barriers constrained by the pre-trained priors and could lead to high-quality adaptation to unseen data. We realize our
          framework using the implicit surface representation and validate the efficacy of our approach in a variety of challenging tasks that take
          highly sparse or collapsed observations as input.
          Experimental results show that our approach compares favorably with the state-of-the-art methods in terms of both generality and accuracy. 
        </p>
    </div>
    
     
    <div class="section">
        <h3>What is this the best strategyof using the prior in a 3D learning task?</h3>
        <hr>
        <img src="teaser.PNG" style="width:50%; display:block; margin-right:auto; margin-left:auto; margin-top:-10px;">
        <p>
                The shape prior learned from the limited training data cannot capture the full landscape of the real data distribution. 
                Common practice that uses a fixed pre-trained generator is constrained within the prior (path a) and thus fails to model 
          the unseen data lying outside the prior, even with latent code optimization at test time. 
          Optimizing a randomly initialized generator, on the other hand, is prone to be trapped in a local minimum due to the complex energy landscape (path b). 
          </p>
        <p>
           Whereas the pre-train prior could provide a good initialization in a forward pass, we propose to further optimize the parameters of the prior and the latent code according to the task-specific constraints at test time. We show in this work that the proposed framework can effectively break the barriers of pre-trained prior and generalize to the unseen data that is out of the prior domain (path c). 
    Hence, our approach can generate results (ending point of path c) closest to the ground truth (star point on the real data manifold) compared to the other learning methods (path a and b).
    
        </p>
    </div>
 <!--
    <div class="section">
        <h3>DeepVoxels: A 3D-structured Neural Scene Representation</h3>
        <hr>
        <p>
            With DeepVoxels, we introduce a 3D-structured neural scene representation.
            DeepVoxels encodes the view-dependent appearance of a 3D scene without having to explicitly model its geometry.
            DeepVoxels is based on a Cartesian 3D grid of persistent features that learn to make use of the underlying 3D
            scene structure. It combines insights from 3D computer vision with recent advances in learning
            image-to-image mappings. DeepVoxels is supervised, without requiring a 3D reconstruction of the scene,
            using a 2D re-rendering loss and enforces perspective and multi-view geometry in a principled manner.
        </p>
    </div>
  -->
    <div class="section">
        <h3>Results on Real Captures with Nearest Neighbor Comparison</h3>
        <hr>
        <img src="img/1-ours.gif" style="width:32%; margin-right:auto; margin-left:auto; margin-top:-10px;">
        <img src="img/1-gt.gif" style="width:32%; margin-right:auto; margin-left:auto; margin-top:-10px;">
        <img src="img/gt.gif" style="width:32%; margin-right:auto; margin-left:auto; margin-top:-10px;">
         <img src="img/lamp1.gif" style="width:32%; margin-right:auto; margin-left:auto; margin-top:-10px;">
    </div>

    <div class="section">
        <h3>Video</h3>
        <hr>
        <iframe width="984" height="497" src="https://youtu.be/embed/5Plwj6lTnoM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<!--        <iframe width="884" height="497" src="https://www.youtube.com/embed/HM_WsZhoGXw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
    </div> 

    <div class="section">
        <h3>Paper</h3>
        <hr>
        <div>
            <div class="list-group">
                <a href="https://arxiv.org/abs/2012.07241" class="list-group-item">
                    <img src="paper_thumbnails.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div>


    <h3>Bibtex</h3>
    <hr>
    <div class="bibtexsection">
    @article{yang2020deep,
        title={Deep Optimized Priors for 3D Shape Modeling and Reconstruction},
        author={Yang, Mingyue and Wen, Yuxin and Chen, Weikai and Chen, Yongwei and Jia, Kui},
        journal={arXiv preprint arXiv:2012.07241},
        year={2020}
    }
 
    </div>


    
    <hr>
      <footer>
          
           <p>Thanks to Volodymyr Kuleshov for his website template. &copy; 2017</p>
      </footer>

    </div><!--/.container-->
  </body>
</html>
