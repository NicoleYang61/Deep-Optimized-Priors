
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="3D Shape Modeling and Reconstruction.">
    <meta name="author" content="Mingyue Yang, 
                                 Yuxin Wen, 
                                 Weikai Chen, 
                                 Yongwei Chen,
                                 Kui Jia">

    <title>Deep Optimized Priors for 3D Shape Modeling and Reconstruction</title>
    <!-- Bootstrap core CSS -->
    <link href="bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">

    <link rel="icon" href="img/favicon.gif" type="image/gif" >
  </head>

  <body>
    <div class="container">

    <div class="jumbotron">
      <h2>Deep Optimized Priors for 3D Shape Modeling and Reconstruction</h2>
      <p class="abstract">3D Shape Modeling and Reconstruction</p>
      <!--<p iclass="authors">
          <a href="http://stanford.edu/~sitzmann/">Vincent Sitzmann</a>,
          <a href="https://niessnerlab.org/members/justus_thies/profile.html">Justus Thies</a>,
          <a href="https://scholar.google.com/citations?user=gRqzSHsAAAAJ&hl=en&oi=ao">Felix Heide</a>,
          <a href="https://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Niessner</a>,
          <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>,
          <a href="http://zollhoefer.com/">Michael Zollh√∂fer</a>
      </p>-->

      <p>
        <a class="btn btn-primary" href="#">Code</a>
        <a class="btn btn-primary" href="#">Dataset</a>
        <a class="btn btn-primary" href="https://arxiv.org/abs/2012.07241">Paper</a>
        <a class="btn btn-primary" href="https://drive.google.com/file/d/1BnZRyNcVUty6-LxAstN83H79ktUq8Cjp/view?usp=sharing">Supplemental Material</a> </p>
    </div>

    <div class="section">
        <h3>Abstract</h3>
        <hr>
        <p>
           Many learning-based approaches have difficulty scaling to unseen data, 
          as the generality of its learned prior is limited to the scale and variations of the training samples. 
          This holds particularly true with 3D learning tasks, given the sparsity of 3D datasets available. 
          We introduce a new learning framework for 3D modeling and reconstruction that greatly improves the generalization 
          ability of a deep generator. Our approach strives to connect the good ends of both learning-based and optimization-based methods. 
          In particular, unlike the common practice that fixes the pre-trained priors at test time, we propose to further optimize the learned
          prior and latent code according to the input physical measurements after the training. We show that the proposed strategy effectively 
          breaks the barriers constrained by the pre-trained priors and could lead to high-quality adaptation to unseen data. We realize our
          framework using the implicit surface representation and validate the efficacy of our approach in a variety of challenging tasks that take
          highly sparse or collapsed observations as input.
          Experimental results show that our approach compares favorably with the state-of-the-art methods in terms of both generality and accuracy. 
        </p>
    </div>
    
      <!--
    <div class="section">
        <h3>2D Generative Models don't Understand 3D</h3>
        <hr>
        <img src="img/teaser.gif" style="width:50%; display:block; margin-right:auto; margin-left:auto; margin-top:-10px;">
        <p>
            Deep Generative Models today allow us to perform highly-realistic image synthesis.
            While each generated image is of high quality, a major challenge is to generate a series of coherent views of
            the same scene. This requires the network to have a latent space representation that fundamentally understands the 3D layout
            of the scene; e.g., how would the same chair look from a different viewpoint?
        </p>
        <p>
            Unfortunately, this is challenging for existing models that are based on a series of 2D convolution kernels.
            Instead of parameterizing 3D transformations, they will explain training data in
            a higher-dimensional feature space, leading to poor generalization to novel views at test time - such as the output of
            Pix2Pix trained on images of the cube above.
        </p>
    </div>

    <div class="section">
        <h3>DeepVoxels: A 3D-structured Neural Scene Representation</h3>
        <hr>
        <p>
            With DeepVoxels, we introduce a 3D-structured neural scene representation.
            DeepVoxels encodes the view-dependent appearance of a 3D scene without having to explicitly model its geometry.
            DeepVoxels is based on a Cartesian 3D grid of persistent features that learn to make use of the underlying 3D
            scene structure. It combines insights from 3D computer vision with recent advances in learning
            image-to-image mappings. DeepVoxels is supervised, without requiring a 3D reconstruction of the scene,
            using a 2D re-rendering loss and enforces perspective and multi-view geometry in a principled manner.
        </p>
    </div>
  -->
    <div class="section">
        <h3>Results on Real Captures with Nearest Neighbor Comparison</h3>
        <hr>
        <img src="img/dyck_stacked.mp4.gif" style="width:32%; margin-right:auto; margin-left:auto; margin-top:-10px;">
        <img src="img/fountain_stacked.mp4.gif" style="width:32%; margin-right:auto; margin-left:auto; margin-top:-10px;">
        <img src="img/globe_stacked.mp4.gif" style="width:32%; margin-right:auto; margin-left:auto; margin-top:-10px;">
    </div>

    <div class="section">
        <h3>Video</h3>
        <hr>
        <iframe width="884" height="497" src="https://youtu.be/5Plwj6lTnoM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<!--        <iframe width="884" height="497" src="https://www.youtube.com/embed/HM_WsZhoGXw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
    </div>

    <div class="section">
        <h3>Paper</h3>
        <hr>
        <div>
            <div class="list-group">
                <a href="https://arxiv.org/abs/2012.07241" class="list-group-item">
                    <img src="img/paper_thumbnails.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div>


    <h3>Bibtex</h3>
    <hr>
    <div class="bibtexsection">
    @article{yang2020deep,
        title={Deep Optimized Priors for 3D Shape Modeling and Reconstruction},
        author={Yang, Mingyue and Wen, Yuxin and Chen, Weikai and Chen, Yongwei and Jia, Kui},
        journal={arXiv preprint arXiv:2012.07241},
        year={2020}
    }
 
    </div>


    
    <hr>
      <footer>
          
           <p>Thanks to Volodymyr Kuleshov for his website template. &copy; 2017</p>
      </footer>

    </div><!--/.container-->
  </body>
</html>
